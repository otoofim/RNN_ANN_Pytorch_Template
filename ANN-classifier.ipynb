{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import zipfile\n",
    "import os\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import rarfile, csv\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data_utils\n",
    "from scipy.stats import pearsonr\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from random import shuffle\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix  \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import sklearn\n",
    "import skorch\n",
    "import pickle\n",
    "import math\n",
    "from collections import Counter\n",
    "from fastprogress import master_bar, progress_bar\n",
    "from sklearn.utils import resample\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from skorch.utils import params_for\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "warnings.filterwarnings(action='once')\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "epoch_num = 400\n",
    "learning_rate = 0.00001\n",
    "#dropout_p = 0.5\n",
    "n_features = len(dataset_encoded_normalized.columns)-1\n",
    "use_cuda = True\n",
    "#weight_decay = 0.0001\n",
    "BATCH_SIZE = 50\n",
    "L2 = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_features, output_size):\n",
    "\n",
    "        super(ANN, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(n_features, 64).cuda()\n",
    "        self.fc2 = nn.Linear(64, 128).cuda()\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(128, 256).cuda()\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc4 = nn.Linear(256, 128).cuda()\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.fc5 = nn.Linear(128, 32).cuda()\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.fc6 = nn.Linear(32, output_size).cuda()\n",
    "        self.softmax = nn.LogSoftmax(dim=1).cuda()\n",
    "\n",
    "\n",
    "\n",
    "    def one_step_forward(self, input):\n",
    "\n",
    "        out = self.fc1(input)\n",
    "        out = self.relu2(self.fc2(out))\n",
    "        out = self.relu3(self.fc3(out))\n",
    "        out = self.relu4(self.fc4(out))\n",
    "        out = self.relu5(self.fc5(out))\n",
    "        prob = self.fc6(out)\n",
    "        \n",
    "        softmax = self.softmax(prob)\n",
    "\n",
    "        return softmax\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \n",
    "    def __init__(self, optimizer = torch.optim.Adam, learning_rate = learning_rate, L2 = L2,\n",
    "                                                    n_features = n_features,\n",
    "                                                    output_size = output_size,\n",
    "                                                    #dropout_p = dropout_p,\n",
    "                                                    criterion = nn.NLLLoss()):\n",
    "        \n",
    "        self.module = ANN(n_features = n_features, output_size = output_size)\n",
    "        self.n_features = n_features\n",
    "        self.criterion = criterion\n",
    "        \n",
    "        self.optimizer_adam = optimizer(self.module.parameters(), lr = learning_rate, weight_decay = L2)\n",
    "        \n",
    "        \n",
    "    def train_step(self, Xi, yi):\n",
    "\n",
    "        loss = None\n",
    "        accuracy = None\n",
    "                \n",
    "        self.module.zero_grad()\n",
    "                    \n",
    "        output = self.module.one_step_forward(torch.tensor(Xi).type(torch.FloatTensor).to(device=device))        \n",
    "            \n",
    "        probabilities = torch.exp(output)\n",
    "        \n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        \n",
    "        if  len(yi) != 0:\n",
    "            \n",
    "            correct = (predicted.squeeze(0) == torch.tensor(yi).type(torch.LongTensor).to(device=device)).sum()\n",
    "            accuracy = correct\n",
    "            loss = self.get_loss(output, yi)\n",
    "        \n",
    "            if self.module.training:\n",
    "\n",
    "                loss.backward()\n",
    "                self.optimizer_adam.step()\n",
    "\n",
    "        return {'loss':  loss, 'probabilities': probabilities, 'acc': accuracy}\n",
    "\n",
    "    def infer(self, Xi, yi=[]):\n",
    "\n",
    "        self.module.eval()\n",
    "        output = self.train_step(Xi, yi)\n",
    "        self.module.train()\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def get_loss(self, ys_pred, y_true):\n",
    "        \n",
    "        return self.criterion(ys_pred, torch.tensor(y_true).type(torch.LongTensor).to(device = device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fault_predictor = Trainer()\n",
    "mb = master_bar(range(1, epoch_num))\n",
    "mb2 = master_bar(range(1, epoch_num))\n",
    "mb.names = ['Training Error', 'Validation Error']\n",
    "mb2.names = ['Training acc', 'Validation acc']\n",
    "\n",
    "\n",
    "train_batch_num = round(len(X_train.to_numpy()) / BATCH_SIZE)\n",
    "test_batch_num = round(len(X_test.to_numpy()) / BATCH_SIZE)\n",
    "\n",
    "\n",
    "epoch_tra_acc = {}\n",
    "epoch_tra_error = {}\n",
    "\n",
    "epoch_evl_acc = {}\n",
    "epoch_evl_error = {}\n",
    "\n",
    "for i in mb:\n",
    "    \n",
    "\n",
    "    X_train_batch = batch_maker(X_train.to_numpy(), BATCH_SIZE)\n",
    "    y_train_batch = batch_maker(y_train.to_numpy(), BATCH_SIZE)\n",
    "    x_test_batch = batch_maker(X_test.to_numpy(), BATCH_SIZE)\n",
    "    y_test_batch = batch_maker(y_test.to_numpy(), BATCH_SIZE)\n",
    "    \n",
    "    tra_acc = []\n",
    "    tra_error = []\n",
    "    \n",
    "    evl_acc = []\n",
    "    evl_error = []\n",
    "    \n",
    "    \n",
    "    for j in progress_bar(range(train_batch_num), parent=mb):\n",
    "        \n",
    "        mb.child.comment = f'training'\n",
    "        \n",
    "        X_tra = next(X_train_batch)\n",
    "        y_tra = next(y_train_batch)\n",
    "        stat = fault_predictor.train_step(X_tra, y_tra)\n",
    "        tra_acc.append((stat['acc'].cpu().detach().numpy())/len(X_tra))\n",
    "        tra_error.append((stat['loss'].cpu().detach().numpy())/len(X_tra))\n",
    "        #print(tra_error[-1])\n",
    "        #print(tra_acc[-1])\n",
    "        #break\n",
    "        \n",
    "    mb.write(\"### Training ### Epoch:{} loss:{} acc:{}\".format(i , round(np.sum(tra_error)/(len(tra_error)*1.), 5), round(np.sum(tra_acc)/(len(tra_acc)*1.), 5)))\n",
    "    epoch_tra_error[i] = round(np.sum(tra_error)/(len(tra_error)*1.), 5)\n",
    "    epoch_tra_acc[i] = (round(np.sum(tra_acc)/(len(tra_acc)*1.), 5))\n",
    "    #graphs_acc = [[epoch_tra_error.keys(), epoch_tra_error.values()],[epoch_evl_error.keys(), epoch_evl_error.values()]]\n",
    "    graphs_acc = [[epoch_tra_error.keys(), epoch_tra_error.values()],[epoch_evl_error.keys(), epoch_evl_error.values()]]\n",
    "    graphs_acc2 = [[epoch_tra_acc.keys(), epoch_tra_acc.values()],[epoch_evl_acc.keys(), epoch_evl_acc.values()]]\n",
    "    mb.update_graph(graphs_acc)\n",
    "    mb2.update_graph(graphs_acc2)\n",
    "\n",
    "    \n",
    "    if (i % 2) == 0:\n",
    "\n",
    "        for k in progress_bar(range(test_batch_num), parent=mb):\n",
    "            \n",
    "            mb.child.comment = f'validation'\n",
    "            \n",
    "            X_tes = next(x_test_batch)\n",
    "            y_tes = next(y_test_batch)\n",
    "            \n",
    "            stat = fault_predictor.infer(X_tes, y_tes)\n",
    "            evl_acc.append((stat['acc'].cpu().detach().numpy())/len(X_tes)) \n",
    "            evl_error.append((stat['loss'].cpu().detach().numpy())/len(X_tes))\n",
    "        mb.write(\"### Validation ### Epoch:{} loss:{} acc:{}\".format(i , round(np.sum(evl_error)/(len(evl_error)*1.), 5), (round(np.sum(evl_acc)/(len(evl_acc)*1.), 5))))\n",
    "        \n",
    "        epoch_evl_acc[i] = (round(np.sum(evl_acc)/(len(evl_acc)*1.), 5))\n",
    "        epoch_evl_error[i] = round(np.sum(evl_error)/(len(evl_error)*1.), 5)\n",
    "        \n",
    "        #graphs_acc = [[epoch_tra_error.keys(), epoch_tra_error.values()], [epoch_evl_error.keys(), epoch_evl_error.values()]]\n",
    "        graphs_acc = [[epoch_tra_error.keys(), epoch_tra_error.values()],[epoch_evl_error.keys(), epoch_evl_error.values()]]\n",
    "        graphs_acc2 = [[epoch_tra_acc.keys(), epoch_tra_acc.values()],[epoch_evl_acc.keys(), epoch_evl_acc.values()]]\n",
    "        mb.update_graph(graphs_acc)\n",
    "        mb2.update_graph(graphs_acc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\administrator\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "pred = fault_predictor.infer(torch.tensor(X_test.to_numpy()).type(torch.FloatTensor).to(device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, predicted = torch.max(pred['probabilities'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr = classification_report(y_test, predicted.cpu().detach().numpy())\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_conf_mat(conf_arr):\n",
    "    norm_conf = []\n",
    "    for i in conf_arr:\n",
    "        a = 0\n",
    "        tmp_arr = []\n",
    "        a = sum(i, 0)\n",
    "        for j in i:\n",
    "            tmp_arr.append(float(j)/float(a))\n",
    "        norm_conf.append(tmp_arr)\n",
    "\n",
    "    fig = plt.figure(figsize=(8, 6))\n",
    "    plt.clf()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_aspect(1)\n",
    "    res = ax.imshow(np.array(norm_conf), cmap=plt.cm.jet, \n",
    "                    interpolation='nearest')\n",
    "\n",
    "    width, height = conf_arr.shape\n",
    "\n",
    "    for x in range(width):\n",
    "        for y in range(height):\n",
    "            ax.annotate(str(conf_arr[x][y]), xy=(y, x), \n",
    "                        horizontalalignment='center',\n",
    "                        verticalalignment='center')\n",
    "\n",
    "    cb = fig.colorbar(res)\n",
    "    alphabet = list(PROM_CAUSE.keys())\n",
    "    plt.xticks(range(width), alphabet[:width],rotation='vertical')\n",
    "    plt.yticks(range(height), alphabet[:height])\n",
    "    plt.savefig('confusion_matrix.png', format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix  \n",
    "\n",
    "\n",
    "conf_arr = confusion_matrix(y_test.astype(int), predicted.cpu().detach().numpy())\n",
    "\n",
    "vis_conf_mat(conf_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
